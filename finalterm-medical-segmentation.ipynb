{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11869322,"sourceType":"datasetVersion","datasetId":7458901},{"sourceId":12120260,"sourceType":"datasetVersion","datasetId":7631646},{"sourceId":429836,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":350376,"modelId":371624}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Cài đặt và import thư viện**","metadata":{}},{"cell_type":"code","source":"!pip install albumentations\n!pip install timm\n!pip install segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:37:25.907226Z","iopub.execute_input":"2025-06-11T10:37:25.907462Z","iopub.status.idle":"2025-06-11T10:38:51.843264Z","shell.execute_reply.started":"2025-06-11T10:37:25.907430Z","shell.execute_reply":"2025-06-11T10:38:51.842519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport glob\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport os\nfrom tqdm.auto import tqdm\nimport torch.nn.functional as F\nimport timm\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport gc\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.metrics import get_stats, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:38:51.844262Z","iopub.execute_input":"2025-06-11T10:38:51.844530Z","iopub.status.idle":"2025-06-11T10:39:04.252446Z","shell.execute_reply.started":"2025-06-11T10:38:51.844505Z","shell.execute_reply":"2025-06-11T10:39:04.251889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.254181Z","iopub.execute_input":"2025-06-11T10:39:04.254537Z","iopub.status.idle":"2025-06-11T10:39:04.314678Z","shell.execute_reply.started":"2025-06-11T10:39:04.254519Z","shell.execute_reply":"2025-06-11T10:39:04.314089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Thiết lập seed (tăng khả năng reproduce)**","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    # torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(31)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.318230Z","iopub.execute_input":"2025-06-11T10:39:04.318755Z","iopub.status.idle":"2025-06-11T10:39:04.340107Z","shell.execute_reply.started":"2025-06-11T10:39:04.318718Z","shell.execute_reply":"2025-06-11T10:39:04.339584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Khởi tạo batch size và epochs**","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 4\nEPOCHS = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.340702Z","iopub.execute_input":"2025-06-11T10:39:04.340927Z","iopub.status.idle":"2025-06-11T10:39:04.352645Z","shell.execute_reply.started":"2025-06-11T10:39:04.340911Z","shell.execute_reply":"2025-06-11T10:39:04.351980Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Augmentation**\n* Train Transform:\nÁp dụng nhiều kỹ thuật biến đổi ảnh để tạo ra các biến thể khác nhau từ cùng một ảnh gốc:\n\nResize: chuẩn hóa kích thước ảnh về 512x512.\n\nHorizontalFlip / VerticalFlip / RandomRotate90: tạo các góc nhìn khác nhau cho ảnh.\n\nShiftScaleRotate: dịch chuyển, phóng to/thu nhỏ, xoay ảnh nhẹ → tăng độ đa dạng hình học.\n\nColor transforms (Brightness, Contrast, Hue, CLAHE): thay đổi màu sắc → mô hình không phụ thuộc màu quá nhiều.\n\nNoise & Blur: mô phỏng điều kiện ảnh chụp kém chất lượng.\n\nDistortion & Dropout: biến dạng và che một phần ảnh → giúp mô hình học được các đặc trưng quan trọng.\n\nNormalize & ToTensor: chuẩn hóa giá trị pixel và chuyển ảnh về tensor để đưa vào mô hình.\n\n* Test Transform:\n\nResize: giữ đồng nhất kích thước ảnh.\n\nNormalize & ToTensor: chuẩn hóa giá trị và chuyển đổi định dạng dữ liệu.","metadata":{}},{"cell_type":"code","source":"trainsize = 512\n\ntrain_transform = A.Compose([\n    A.Resize(width=trainsize, height=trainsize),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, border_mode=0, p=0.5), \n\n    A.OneOf([\n        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        A.CLAHE(),\n        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n    ], p=0.5),\n\n    A.OneOf([\n        A.GaussNoise(var_limit=(10.0, 50.0)),\n        A.MotionBlur(blur_limit=3),\n        A.MedianBlur(blur_limit=3),\n    ], p=0.5),\n    \n    A.OneOf([\n        A.ElasticTransform(p=0.3),\n        A.GridDistortion(p=0.3),\n        A.OpticalDistortion(p=0.3),\n    ], p=0.3),\n    \n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5),\n\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntest_transform = A.Compose([\n    A.Resize(width=trainsize, height=trainsize),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),\n    ToTensorV2(), \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.353416Z","iopub.execute_input":"2025-06-11T10:39:04.353577Z","iopub.status.idle":"2025-06-11T10:39:04.379894Z","shell.execute_reply.started":"2025-06-11T10:39:04.353564Z","shell.execute_reply":"2025-06-11T10:39:04.379138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Chia tập train val**\nLớp SkinDataset chia dữ liệu thành 90% để huấn luyện và 10% để validation bằng train_test_split.\nViệc chia này giúp mô hình học từ tập huấn luyện và được đánh giá công bằng trên tập validation — đảm bảo không đánh giá trên dữ liệu đã thấy.","metadata":{}},{"cell_type":"code","source":"class SkinDataset(Dataset):\n    def __init__(self, images_path, masks_path, split, val_split = 0.2,transform=None):\n        super().__init__()\n        self.images_path = images_path\n        self.masks_path = masks_path\n        all_images = sorted(os.listdir(images_path))\n        all_masks = sorted(os.listdir(masks_path))\n        \n        train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n            all_images, all_masks, test_size=val_split, random_state=33\n        )\n\n        if split == 'train':\n            self.images = train_imgs\n            self.masks = train_masks\n        else:\n            self.images = val_imgs\n            self.masks = val_masks\n            \n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def mask_to_binary(self, mask):\n        if mask.ndim == 3:\n            mask = mask[:, :, 0]\n        return (mask == 255).astype(np.float32)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.images_path, self.images[idx])\n        mask_path = os.path.join(self.masks_path, self.masks[idx])\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        mask = cv2.imread(mask_path)\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n        mask = self.mask_to_binary(mask)\n\n        if self.transform is not None:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.380795Z","iopub.execute_input":"2025-06-11T10:39:04.381094Z","iopub.status.idle":"2025-06-11T10:39:04.395822Z","shell.execute_reply.started":"2025-06-11T10:39:04.381070Z","shell.execute_reply":"2025-06-11T10:39:04.395176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"traindataset = SkinDataset(images_path = \"/kaggle/input/medical-segmentation/Dataset/Train/Image\",\n                          masks_path = \"/kaggle/input/medical-segmentation/Dataset/Train/Mask\",\n                          split = 'train',\n                          transform = train_transform)\n\nvaldataset = SkinDataset(images_path = \"/kaggle/input/medical-segmentation/Dataset/Train/Image\",\n                          masks_path = \"/kaggle/input/medical-segmentation/Dataset/Train/Mask\",\n                          split = 'val',\n                          transform = test_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.398294Z","iopub.execute_input":"2025-06-11T10:39:04.398496Z","iopub.status.idle":"2025-06-11T10:39:04.518368Z","shell.execute_reply.started":"2025-06-11T10:39:04.398480Z","shell.execute_reply":"2025-06-11T10:39:04.517876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainloader = DataLoader(traindataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2, drop_last = True)\nvalloader = DataLoader(valdataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 2, drop_last = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.518811Z","iopub.execute_input":"2025-06-11T10:39:04.519014Z","iopub.status.idle":"2025-06-11T10:39:04.523142Z","shell.execute_reply.started":"2025-06-11T10:39:04.518999Z","shell.execute_reply":"2025-06-11T10:39:04.522286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(trainloader), len(valloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.523917Z","iopub.execute_input":"2025-06-11T10:39:04.524152Z","iopub.status.idle":"2025-06-11T10:39:04.541562Z","shell.execute_reply.started":"2025-06-11T10:39:04.524138Z","shell.execute_reply":"2025-06-11T10:39:04.541038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Hiển thị ảnh và mask sau khi augment**","metadata":{}},{"cell_type":"code","source":"test_img, test_mask = next(iter(trainloader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:04.542297Z","iopub.execute_input":"2025-06-11T10:39:04.542494Z","iopub.status.idle":"2025-06-11T10:39:05.818066Z","shell.execute_reply.started":"2025-06-11T10:39:04.542480Z","shell.execute_reply":"2025-06-11T10:39:05.817213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n        \n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n            # The normalize code -> t.sub_(m).div_(s)\n        return tensor\n    \nunorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:05.819172Z","iopub.execute_input":"2025-06-11T10:39:05.819435Z","iopub.status.idle":"2025-06-11T10:39:05.824727Z","shell.execute_reply.started":"2025-06-11T10:39:05.819409Z","shell.execute_reply":"2025-06-11T10:39:05.824013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_batch(images, masks, unorm, max_images=8):\n    \"\"\"\n    images: Tensor of shape [B, C, H, W]\n    masks: Tensor of shape [B, H, W]\n    unorm: function to denormalize image\n    \"\"\"\n    batch_size = min(images.size(0), max_images)\n    plt.figure(figsize=(6, batch_size * 3))\n\n    for i in range(batch_size):\n        img = unorm(images[i]).permute(1, 2, 0).cpu().numpy()\n        mask = masks[i].cpu().numpy()\n\n        # Image subplot\n        plt.subplot(batch_size, 2, 2 * i + 1)\n        plt.imshow(img)\n        plt.title(f\"Image {i}\")\n        plt.axis('off')\n\n        # Mask subplot\n        plt.subplot(batch_size, 2, 2 * i + 2)\n        plt.imshow(mask, cmap='gray')\n        plt.title(f\"Mask {i}\")\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:05.825489Z","iopub.execute_input":"2025-06-11T10:39:05.825690Z","iopub.status.idle":"2025-06-11T10:39:05.842632Z","shell.execute_reply.started":"2025-06-11T10:39:05.825667Z","shell.execute_reply":"2025-06-11T10:39:05.842058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_batch(test_img, test_mask, unorm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:05.843293Z","iopub.execute_input":"2025-06-11T10:39:05.843530Z","iopub.status.idle":"2025-06-11T10:39:06.813631Z","shell.execute_reply.started":"2025-06-11T10:39:05.843511Z","shell.execute_reply":"2025-06-11T10:39:06.812794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Mô hình huấn luyện**\n### Kiến trúc tổng quan\n\nMô hình là một phiên bản cải tiến của kiến trúc U-Net cổ điển, sử dụng **ResNeSt269e** làm backbone encoder để trích xuất đặc trưng mạnh mẽ. Decoder được thiết kế bằng các khối `DoubleConv` đơn giản nhưng hiệu quả.\n\n### Chi tiết kiến trúc\n\n- **Encoder (ResNeSt269e pretrained)**: Trích xuất 5 mức đặc trưng `x1 → x5`, với độ sâu và số kênh tăng dần.\n- **Bottleneck**: `DoubleConv(2048 → 1024)` xử lý đặc trưng từ tầng sâu nhất.\n- **Decoder**:\n  - 4 tầng giải mã.\n  - Mỗi tầng: Upsample → Ghép nối với skip connection từ encoder → `DoubleConv`.\n- **Output**: `Conv2d(64 → n_classes)` → Upsample cuối cùng để khôi phục kích thước ảnh gốc.\n\n```mermaid\ngraph TD\n    Input[Input Image (3xHxW)]\n    E1[Encoder Stage 1 (x1): C=64]\n    E2[Encoder Stage 2 (x2): C=128]\n    E3[Encoder Stage 3 (x3): C=256]\n    E4[Encoder Stage 4 (x4): C=1024]\n    E5[Encoder Stage 5 (x5): C=2048]\n    Neck[DoubleConv(2048 → 1024)]\n\n    U1[Decoder Up1: Cat(x4, ↑x) → DoubleConv(2048 → 512)]\n    U2[Decoder Up2: Cat(x3, ↑x) → DoubleConv(1024 → 256)]\n    U3[Decoder Up3: Cat(x2, ↑x) → DoubleConv(512 → 128)]\n    U4[Decoder Up4: Cat(x1, ↑x) → DoubleConv(256 → 64)]\n\n    CLS[Conv2d(64 → n_classes)]\n    Out[Upsample → Final Output (HxW)]\n\n    Input --> E1 --> E2 --> E3 --> E4 --> E5 --> Neck\n    Neck --> U1 --> U2 --> U3 --> U4 --> CLS --> Out\n```\n\n\nƯu điểm trong bài toán segmentation:\n* Giữ lại thông tin không gian: Nhờ skip connections, mô hình có thể phục hồi chi tiết vùng biên và cấu trúc đối tượng.\n* Biểu diễn đặc trưng mạnh mẽ: Sử dụng ResNeSt269e, một backbone hiện đại với self-attention giúp mô hình học tốt hơn các đặc trưng không gian-phân bố.\n* Decoder nhẹ nhưng hiệu quả: Các tầng DoubleConv và Upsample giúp khôi phục phân giải nhanh chóng mà không tốn quá nhiều tài nguyên.","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\nclass Unet_Modern(nn.Module):\n    def __init__(self, n_classes = 1):\n        super().__init__()\n        self.n_classes = n_classes\n        self.encoder = timm.create_model(\"resnest269e\", pretrained=True, features_only=True)\n        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n        self.block_neck = DoubleConv(2048, 1024)\n        self.block_up1 = DoubleConv(1024+1024, 512)\n        self.block_up2 = DoubleConv(512+512, 256)\n        self.block_up3 = DoubleConv(256+256, 128)\n        self.block_up4 = DoubleConv(128+128, 64)\n        self.conv_cls = nn.Conv2d(64, self.n_classes, 1)\n\n    def forward(self, x):\n        # skip connections của resnet50\n        x1, x2, x3, x4, x5 = self.encoder(x)\n        # print(x1.shape)\n        # print(x2.shape)\n        # print(x3.shape)\n        # print(x4.shape)\n        # print(x5.shape)\n\n        # bottleneck\n        x = self.block_neck(x5) # x (B, 1024, 8, 8)\n\n        # concat skip connections then halve the features channels\n        x = torch.cat([x4, self.upsample(x)], dim=1) \n        # print(\"before block1\", x.shape)\n        x = self.block_up1(x)\n        x = torch.cat([x3, self.upsample(x)], dim=1)\n        # print(\"before block2\",x.shape)\n        x = self.block_up2(x)\n        x = torch.cat([x2, self.upsample(x)], dim=1)\n        # print(\"before block3\",x.shape)\n        x = self.block_up3(x)\n        x = torch.cat([x1, self.upsample(x)], dim=1)\n        # print(\"before block4\", x.shape)\n        x = self.block_up4(x)\n        # classifier\n        x = self.conv_cls(x) #size/2\n        # scaling to return to the original size\n        x = self.upsample(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:06.814517Z","iopub.execute_input":"2025-06-11T10:39:06.814771Z","iopub.status.idle":"2025-06-11T10:39:06.823809Z","shell.execute_reply.started":"2025-06-11T10:39:06.814741Z","shell.execute_reply":"2025-06-11T10:39:06.823078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Kết hợp Dice Loss và Focal Loss**\nDice Loss: tập trung tối đa vào vùng foreground (vùng có mask), hiệu quả trong xử lý mất cân bằng giữa nền và vật thể.\n\nFocal Loss: giảm ảnh hưởng từ các mẫu dễ, tăng trọng số cho các điểm khó phân loại (vùng ranh giới, nhiễu).","metadata":{}},{"cell_type":"code","source":"class DiceFocalLoss(nn.Module):\n    def __init__(self, alpha=0.7, beta=0.3):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.focal = smp.losses.FocalLoss(mode='binary', gamma=2.0)\n        self.dice = smp.losses.DiceLoss(mode='binary', from_logits=True)\n\n    def forward(self, pred, target):\n        target = target.unsqueeze(1)  # đảm bảo shape là [B, 1, H, W]\n        return self.alpha * self.dice(pred, target) + self.beta * self.focal(pred, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:06.824584Z","iopub.execute_input":"2025-06-11T10:39:06.824941Z","iopub.status.idle":"2025-06-11T10:39:06.844628Z","shell.execute_reply.started":"2025-06-11T10:39:06.824919Z","shell.execute_reply":"2025-06-11T10:39:06.844012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Hàm tính Dice coefficient**","metadata":{}},{"cell_type":"code","source":"def calc_f1(output, target, threshold=0.5):\n    tp, fp, fn, tn = get_stats(output, target.unsqueeze(dim=1), mode='binary', threshold=threshold)\n    f1 = f1_score(tp, fp, fn, tn)\n    return f1.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:06.845280Z","iopub.execute_input":"2025-06-11T10:39:06.845543Z","iopub.status.idle":"2025-06-11T10:39:06.862471Z","shell.execute_reply.started":"2025-06-11T10:39:06.845521Z","shell.execute_reply":"2025-06-11T10:39:06.861884Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Hàm huấn luyện một epoch**","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, trainloader, scaler, optimizer, loss_fn, device=device):\n    model.train()\n    pbar = tqdm(enumerate(trainloader), leave=True)\n    total_loss = 0.0\n    total_score = 0.0\n\n    for batch_idx, (img, target_mask) in pbar:\n        img, target_mask = img.to(device), target_mask.to(device)\n\n        with torch.amp.autocast(device_type=device):\n            logits_mask = model(img)\n            loss = loss_fn(logits_mask, target_mask)\n\n        probs = torch.sigmoid(logits_mask)\n        score = calc_f1(probs, target_mask.to(torch.long))\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n        total_score += score.item()\n        avg_loss = total_loss / (batch_idx + 1)\n        avg_score = total_score / (batch_idx + 1)\n\n        pbar.set_postfix(avg_loss=avg_loss, dice_score=avg_score)\n\n    print(f\"Average loss: {avg_loss:.4f}, Average dice score: {avg_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:06.863177Z","iopub.execute_input":"2025-06-11T10:39:06.863404Z","iopub.status.idle":"2025-06-11T10:39:06.879054Z","shell.execute_reply.started":"2025-06-11T10:39:06.863384Z","shell.execute_reply":"2025-06-11T10:39:06.878373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Tối ưu hoá mô hình**\nAdamW: giống Adam nhưng tách riêng weight decay, giúp regularization tốt hơn, tránh overfitting.\n\nReduceLROnPlateau: giảm learning rate khi validation loss không cải thiện → giúp mô hình học chậm lại khi cần.\n\nGradScaler (AMP): dùng Automatic Mixed Precision (AMP) giúp tăng tốc độ huấn luyện và giảm tiêu thụ VRAM.\n","metadata":{}},{"cell_type":"code","source":"model = Unet_Modern().to(device) \noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-4,\n    # weight_decay=1e-4 \n)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,      \n    patience=3,      \n    threshold=1e-4,  \n    min_lr=1e-12,\n)\nloss_fn = DiceFocalLoss().to(device)\nscaler = torch.amp.GradScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:06.879734Z","iopub.execute_input":"2025-06-11T10:39:06.879968Z","iopub.status.idle":"2025-06-11T10:39:11.189643Z","shell.execute_reply.started":"2025-06-11T10:39:06.879948Z","shell.execute_reply":"2025-06-11T10:39:11.188884Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Kiểm tra tập val trên nhiều threshold**\n* Ta đánh giá F1 (Dice) cho mỗi threshold rồi chọn threshold có điểm số tốt nhất để tăng độ chính xác khi phân đoạn ảnh.\n* Ban đầu em kiểm tra trong khoảng 0.4 đến 0.8 với bước nhảy 0.05 nhưng sau nhiều lần thử nghiệm thì rút gọn lại được khoảng tốt nhất là 0.45 đến 0.55","metadata":{}},{"cell_type":"code","source":"def validate_one_epoch(model, valloader, loss_fn, scheduler, device=device):\n    model.eval()\n    pbar = tqdm(enumerate(valloader), leave=True)\n    total_loss = 0.0\n    # total_score = 0.0\n    thresholds = [0.45, 0.5, 0.55]\n    threshold_scores = {t: 0.0 for t in thresholds}\n    print('current learning rate', scheduler.get_last_lr())\n    with torch.inference_mode():\n        for batch_idx, (img, target_mask) in pbar:\n            img, target_mask = img.to(device), target_mask.to(device)\n\n            with torch.amp.autocast(device_type=device):\n                logits_mask = model(img)\n                loss = loss_fn(logits_mask, target_mask)\n\n            probs = torch.sigmoid(logits_mask)\n            for t in thresholds:\n                score = calc_f1(probs, target_mask.to(torch.long), threshold=t)\n                threshold_scores[t] += score.item()\n            # score = calc_f1(probs, target_mask.to(torch.long))\n\n            total_loss += loss.item()\n            # total_score += score.item()\n            avg_loss = total_loss / (batch_idx + 1)\n            # avg_score = total_score / (batch_idx + 1)\n            avg_scores = {t: threshold_scores[t] / (batch_idx + 1) for t in thresholds}\n            best_threshold = max(avg_scores, key=avg_scores.get)\n            best_score = avg_scores[best_threshold]\n\n            pbar.set_postfix(val_loss=avg_loss, best_dice=best_score)\n\n    scheduler.step(avg_loss)  # Nếu bạn dùng ReduceLROnPlateau\n    # current_lr = scheduler.optimizer.param_groups[0][\"lr\"]\n    # for param_group in scheduler.optimizer.param_groups:\n    #     param_group[\"weight_decay\"] = current_lr\n    # print(f\"current weight decay\", current_lr)\n    print(f\"Validation Average loss: {avg_loss:.4f}, Best dice score: {best_score:.4f} at threshold {best_threshold}\")\n    return avg_loss, best_score, best_threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:11.193211Z","iopub.execute_input":"2025-06-11T10:39:11.193441Z","iopub.status.idle":"2025-06-11T10:39:11.201292Z","shell.execute_reply.started":"2025-06-11T10:39:11.193425Z","shell.execute_reply":"2025-06-11T10:39:11.200220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Huấn luyện mô hình**","metadata":{}},{"cell_type":"code","source":"final_best_dice = float('-inf')\nfinal_best_threshold = 0.5 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:11.202041Z","iopub.execute_input":"2025-06-11T10:39:11.202307Z","iopub.status.idle":"2025-06-11T10:39:12.002651Z","shell.execute_reply.started":"2025-06-11T10:39:11.202285Z","shell.execute_reply":"2025-06-11T10:39:12.001783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in tqdm(range(EPOCHS)):\n    train_one_epoch(model, trainloader, scaler, optimizer, loss_fn)\n    avg_loss, best_score, best_threshold = validate_one_epoch(model, valloader, loss_fn, scheduler)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    if best_score > final_best_dice:\n        torch.save(model.state_dict(), \"best_model.pth\")\n        final_best_dice = best_score\n        final_best_threshold = best_threshold\n        print(f\"✅ Saved Best Model at Epoch {epoch} with dice {best_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T10:39:18.841086Z","iopub.execute_input":"2025-06-11T10:39:18.841384Z","iopub.status.idle":"2025-06-11T12:21:10.444695Z","shell.execute_reply.started":"2025-06-11T10:39:18.841363Z","shell.execute_reply":"2025-06-11T12:21:10.443431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Kiểm tra trên tập test**","metadata":{}},{"cell_type":"code","source":"test_model = Unet_Modern().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T12:22:16.025602Z","iopub.execute_input":"2025-06-11T12:22:16.026251Z","iopub.status.idle":"2025-06-11T12:22:18.859181Z","shell.execute_reply.started":"2025-06-11T12:22:16.026225Z","shell.execute_reply":"2025-06-11T12:22:18.858588Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model tốt nhất trong tất cả các lần huấn luyện, được dùng để sinh csv nộp trên competition**","metadata":{}},{"cell_type":"code","source":"# test_model.load_state_dict(torch.load(\n#     f'/kaggle/input/resnet-hopfully/pytorch/default/1/best_model.pth',\n# ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T12:26:05.861425Z","iopub.execute_input":"2025-06-11T12:26:05.861712Z","iopub.status.idle":"2025-06-11T12:26:11.899419Z","shell.execute_reply.started":"2025-06-11T12:26:05.861693Z","shell.execute_reply":"2025-06-11T12:26:11.898766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model tốt nhất được lưu trong một lần huấn luyện**","metadata":{}},{"cell_type":"code","source":"test_model.load_state_dict(torch.load(f'/kaggle/working/best_model.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T12:22:47.567721Z","iopub.execute_input":"2025-06-11T12:22:47.568420Z","iopub.status.idle":"2025-06-11T12:22:48.484322Z","shell.execute_reply.started":"2025-06-11T12:22:47.568394Z","shell.execute_reply":"2025-06-11T12:22:48.483676Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Sinh file kết quả csv**\n* Biểu diễn mặt nạ nhị phân thành chuỗi số (RLE), phù hợp để nộp kết quả trong các bài toán segmentation\n* Show kết quả segmenatation của mô hình\n* **Kết quả tốt nhất đạt được là 0.9142**","metadata":{}},{"cell_type":"code","source":"folder_path=\"/kaggle/input/medical-segmentation/Dataset/Test\"\noutput_path=\"/kaggle/working/\"\nimage_folder = os.path.join(folder_path, \"Image\")\n\nimage_files = glob.glob(os.path.join(image_folder, \"*.jpg\"))\n\ntest_transform = A.Compose([\n    A.Resize(width=trainsize, height=trainsize),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\nresults = {'ID': [], 'Mask': []}\n\ntest_model.eval()\ntest_model.to(device)\n\nfor img_path in image_files:\n    img_id = os.path.basename(img_path).split('.')[0]\n    \n    image = cv2.imread(img_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_size = (image.shape[1], image.shape[0])\n    \n    transformed = test_transform(image=image)\n    image_tensor = transformed['image'].unsqueeze(0).to(device)\n    \n    with torch.inference_mode():\n        output_mask = test_model(image_tensor)\n    \n    output_mask = output_mask.squeeze().cpu().numpy()  \n    binary_mask = (output_mask > final_best_threshold).astype(np.uint8)\n    \n    pred_mask_resized = cv2.resize(binary_mask, (original_size[0], original_size[1]), \n                                  interpolation=cv2.INTER_NEAREST)\n    \n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    \n    axs[0].imshow(image)\n    axs[0].set_title(f\"Image ID: {img_id}\")\n    axs[0].axis('off')\n\n    axs[1].imshow(pred_mask_resized, cmap='gray')\n    axs[1].set_title(\"Predicted Mask\")\n    axs[1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    mask_rle = mask_to_rle(pred_mask_resized)\n    results['ID'].append(img_id)\n    results['Mask'].append(mask_rle)\n\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values(by='ID', key=lambda x: x.astype(int)).reset_index(drop=True)\n\noutput_file = os.path.join(output_path, \"predictions.csv\")\nresults_df.to_csv(output_file, index=False)\n\nprint(f\"Hoàn thành dự đoán, lưu kết quả vào {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T12:26:18.976238Z","iopub.execute_input":"2025-06-11T12:26:18.976812Z","iopub.status.idle":"2025-06-11T12:28:29.060474Z","shell.execute_reply.started":"2025-06-11T12:26:18.976790Z","shell.execute_reply":"2025-06-11T12:28:29.059883Z"}},"outputs":[],"execution_count":null}]}